# Bias in Link Prediction Benchmarks

<p float="left">
  <img src="analysis/output/network_analysis/visualizations/FB15k-237.png" width="310" />
  <img src="analysis/output/pattern_analysis/YAGO3-10.png" width="210" /> 
  <img src="analysis/output/relation_distribution/YAGO3-10.png" width="250" />
</p>

This repository contains all the work needed to characterize the most benchmarks used for link prediction:

- Analysis of Test Leakage and Sample Selection Bias
- Topological analysis (Visualization images, network metrics)
- Experiments for analysing bias in prediction results in FB15k, FB15k-237, WN18 and WN18RR
- Mappings for generating N-Triple files and to upload benchmark data to a knowledge graph
- SPARQL queries for analyzing bias patterns and other irregularities

We analyze the following benchmark datasets in this work:

- FB15k
- FB15k-237
- WN18
- WN18RR
- YAGO3-10
- Wikidata5M
- DBpedia50k

## Data files

All datasets - except Wikidata5M - can be found in the `data` folder. [Wikidata5M](https://deepgraphlearning.github.io/project/wikidata5m) needs to be manually imported due its large size. It can be directly downloaded from [here](https://www.dropbox.com/s/6sbhm0rwo4l73jq/wikidata5m_transductive.tar.gz?dl=1). The CSV files can be generated by placing the text files under data/Wikidata5M/original and running the script `data/write_csv.py`.

## Dataset statistics

Statistics regarding entities, relations and triples for each split can be found under `analysis/data/split_statistics`.

<p><img src="analysis/output/split_analysis/overview.png" width="600" /></p>

## Topological results

- Relation distributions: `analysis/output/relation_distribution`
- Network visualizations: `analysis/output/network_analysis/visualizations`
- Other network characteristics (degree, components, pagerank, communities): `analysis/output/network_analysis/{partition}`

## Bias analysis

In the thesis, seven bias patterns are defined concerning test leakage and sample selection bias.

Test leakage patterns:

- Near-duplicate relations
- Near-inverse relations
- Near-symmetric relations

For sample selection bias, we reused patterns defined in the work of [Rossi et al.](https://github.com/merialdo/research.lpbias):

- Overrepresented tail answers (referred to as Type 1 Bias by Rossi et al.)
- Overrepresented head answers
- Default tail answers (referred to as Type 2 Bias by Rossi et al.)
- Default head answers

Using these patterns, we queried the number of bias-affected triples for each split in every dataset with SPARQL. The queries can be found in the folder `sparql/affectedTriples`. The following plots provide a good overview of bias in each benchmark:

<p float="left">
  <img src="analysis/output/pattern_analysis/FB15k.png" width="200" />
  <img src="analysis/output/pattern_analysis/FB15k-237.png" width="200" />
  <img src="analysis/output/pattern_analysis/WN18.png" width="200" />
  <img src="analysis/output/pattern_analysis/WN18RR.png" width="200" />
</p>
<p float="left">
  <img src="analysis/output/pattern_analysis/YAGO3-10.png" width="200" />
  <img src="analysis/output/pattern_analysis/Wikidata5M.png" width="200" />
  <img src="analysis/output/pattern_analysis/DBpedia50k.png" width="200" />
</p>

Following observations can be made:
- FB15k and WN18 suffer from test leakage due to near-inverse relations
- Test leakage can be found in YAGO3-10 through near-duplicate relations (over 63.3% of test triples are near-duplicates in the training set)
- WN18RR contains a large number of near-symmetric relations (almost 40%), higher than WN18
- FB15k-237 features a lot of test triples that have a default tail answer

## Experimental results

To understand how these bias types affect prediction results we then tried to explain each correct prediction (based on H@k metric) by one or more of our bias types on a learned TransE model. If no explanation could be found the correct prediction is assigned to the bucket *unknown*. The jupyter notebook for bucketing predictions can be found under `experiments/prediction_analysis.ipynb`. 

<p float="left">
  <img src="analysis/output/prediction_analysis/fb15k_head.png" width="200" />
  <img src="analysis/output/prediction_analysis/fb15k_tail.png" width="200" />
  <img src="analysis/output/prediction_analysis/fb15k237_head.png" width="200" />
  <img src="analysis/output/prediction_analysis/fb15k237_tail.png" width="200" />
</p>
<p float="left">
  <img src="analysis/output/prediction_analysis/wn18_head.png" width="200" />
  <img src="analysis/output/prediction_analysis/wn18_tail.png" width="200" />
  <img src="analysis/output/prediction_analysis/wn18rr_head.png" width="200" />
  <img src="analysis/output/prediction_analysis/wn18rr_tail.png" width="200" />
</p>

Following observations can be made:
- Correct predictions in FB15k and WN18 can almost completely be explained with our defined bias patterns
- In WN18RR, the majority of correct predictions can be explained with the occurrence of near-symmetric relations
- We further notice that this pattern occurs at a ~100% higher rate than in the input data. This behavior reinforces the idea that the model is biased towards learning these patterns

## Reproducing the experiments

We learn the embeddings using the TransE implementation found in the Python library AmpliGraph (version 2.0.0). We obtain rankings for test triples by training the models FB15k, FB15k-237, WN18 and WN18RR in the notebooks under `experiments/learn_embeddings`. The hyperparameters found [here](https://docs.ampligraph.org/en/latest/experiments.html) were reused for our experiments. 

<p float="left">
  <img src="experiments/hyperparameters.png" width="500" />
</p>
<p float="left">
  <img src="experiments/performance.png" width="500" />
</p>

## How to generate N-Triple files for each dataset?

1. Install the RDFizer using `python3 -m pip install rdfizer`
2. `cd mappings`
3. `bash generate_triple_files.sh`
